# CLUSTERING DE LIBROS - EXPLICACIÃ“N DETALLADA

## Objetivo del Proyecto

Agrupar automÃ¡ticamente 300 libros de Goodreads en categorÃ­as similares usando **Machine Learning no supervisado**.

---

## Â¿QuÃ© es el Clustering?

El **clustering** es una tÃ©cnica de Machine Learning que agrupa elementos similares sin necesidad de etiquetas previas. Es como cuando organizas tu biblioteca: pones los libros de terror juntos, los de romance juntos, etc., pero lo hace la mÃ¡quina automÃ¡ticamente.

### Diferencia vs ClasificaciÃ³n:

- **ClasificaciÃ³n** (supervisado): Necesitas ejemplos etiquetados â†’ "Este libro es de terror, este de romance"
- **Clustering** (no supervisado): La mÃ¡quina encuentra grupos por sÃ­ misma â†’ "Estos libros se parecen entre sÃ­"

---

## TecnologÃ­as Utilizadas

### 1. **TF-IDF** (Term Frequency-Inverse Document Frequency)

Convierte texto en nÃºmeros que las mÃ¡quinas pueden procesar.

#### Â¿CÃ³mo funciona?

**TF (Term Frequency)**: Frecuencia del tÃ©rmino en el documento

```
TF = (NÃºmero de veces que aparece la palabra) / (Total de palabras en el documento)
```

**IDF (Inverse Document Frequency)**: Penaliza palabras muy comunes

```
IDF = log(Total de documentos / Documentos que contienen la palabra)
```

**TF-IDF final**:

```
TF-IDF = TF Ã— IDF
```

#### Ejemplo PrÃ¡ctico:

DescripciÃ³n de "The Hunger Games":

```
"Katniss Everdeen participates in the Hunger Games, a fight to the death..."
```

| Palabra | TF   | IDF  | TF-IDF                          |
| ------- | ---- | ---- | ------------------------------- |
| Katniss | 0.05 | 5.2  | **0.26** (importante)           |
| Games   | 0.03 | 3.1  | **0.09** (importante)           |
| the     | 0.15 | 0.1  | **0.015** (poco importante)     |
| a       | 0.10 | 0.05 | **0.005** (muy poco importante) |

**Resultado**: Las palabras especÃ­ficas como "Katniss" y "Games" tienen mÃ¡s peso que palabras comunes como "the" o "a".

---

### 2. **K-Means Clustering**

Algoritmo que agrupa datos en K clusters (grupos).

#### Proceso paso a paso:

```
1. INICIALIZACIÃ“N
   - Decides K = 6 clusters
   - Se colocan 6 centroides aleatorios

2. ASIGNACIÃ“N
   - Cada libro se asigna al centroide mÃ¡s cercano

3. ACTUALIZACIÃ“N
   - Se recalcula la posiciÃ³n de cada centroide (promedio de sus libros)

4. REPETIR
   - Pasos 2-3 hasta que no haya cambios
```

#### VisualizaciÃ³n:

```
IteraciÃ³n 0 (inicio):
  â—     â—‹
    â—‹     â—
  â—‹   â—
    â—‹     â—‹
  â—     â—‹

IteraciÃ³n 5 (convergencia):
  â— â— â—

  â—‹ â—‹ â—‹

  â—† â—† â—†
```

---

## Estructura del CÃ³digo

### Paso 1: Cargar Datos

```python
df = pd.read_csv("books_google.csv")
# title, author, rating, genres, description
```

### Paso 2: Preprocesar

```python
# Eliminar libros sin descripciÃ³n
df['description'] = df['description'].fillna('')
df = df[df['description'] != '']
```

### Paso 3: TF-IDF

```python
vectorizer = TfidfVectorizer(
    max_features=500,      # Considerar top 500 palabras
    max_df=0.8,           # Ignorar si aparece en >80% libros
    min_df=2,             # Ignorar si aparece en <2 libros
    stop_words='english'  # Eliminar "the", "a", "is"...
)

tfidf_matrix = vectorizer.fit_transform(df['description'])
# Resultado: matriz de 300 libros Ã— 500 palabras
```

### Paso 4: K-Means

```python
kmeans = KMeans(n_clusters=6, random_state=42)
df['cluster'] = kmeans.fit_predict(tfidf_matrix)
```

### Paso 5: Analizar Resultados

```python
# Palabras clave por cluster
for cluster in range(6):
    centroid = kmeans.cluster_centers_[cluster]
    top_words = get_top_words(centroid)
    print(f"Cluster {cluster}: {top_words}")
```

---

## Resultados Esperados

### Ejemplo de Clusters:

**Cluster 0: FantasÃ­a Juvenil**

- The Hunger Games
- Divergent
- Harry Potter
- Palabras clave: _magic, world, young, adventure_

**Cluster 1: ClÃ¡sicos**

- Pride and Prejudice
- Jane Eyre
- Great Expectations
- Palabras clave: _love, society, family, life_

**Cluster 2: Thrillers**

- Gone Girl
- The Girl on the Train
- Shutter Island
- Palabras clave: _murder, mystery, detective, truth_

**Cluster 3: Ciencia FicciÃ³n**

- 1984
- Brave New World
- Fahrenheit 451
- Palabras clave: _future, society, world, control_

**Cluster 4: Romance/Drama**

- The Notebook
- The Fault in Our Stars
- Me Before You
- Palabras clave: _love, life, heart, story_

**Cluster 5: Horror/Terror**

- It
- The Shining
- Dracula
- Palabras clave: _dark, fear, night, death_

---

## Visualizaciones

### 1. **Scatter Plot 2D**

```
Usa PCA (Principal Component Analysis) para reducir
500 dimensiones â†’ 2 dimensiones visualizables

Cada punto = un libro
Color = cluster asignado
```

### 2. **GrÃ¡fico de Barras**

```
Muestra cuÃ¡ntos libros hay en cada cluster
```

---

## ðŸ’¡ Sistema de RecomendaciÃ³n

Una vez tenemos los clusters, podemos recomendar libros:

```python
def recomendar_libros(titulo):
    # 1. Encuentra el libro
    libro = buscar(titulo)

    # 2. ObtÃ©n su cluster
    cluster = libro['cluster']

    # 3. Recomienda otros del mismo cluster
    recomendaciones = libros_en_cluster(cluster)

    return recomendaciones
```

**Ejemplo**:

```
Usuario lee: "Harry Potter"
Cluster: 0 (FantasÃ­a Juvenil)
Recomendaciones:
  â€¢ The Hunger Games
  â€¢ Percy Jackson
  â€¢ Divergent
  â€¢ Eragon
```

---

## CÃ³mo Ejecutar

### InstalaciÃ³n de dependencias:

```bash
pip install pandas numpy scikit-learn matplotlib seaborn
```

### Ejecutar el script:

```bash
python clustering_books.py
```

### Archivos generados:

```
âœ“ books_clustered.csv         â†’ Datos con cluster asignado
âœ“ cluster_summary.csv          â†’ EstadÃ­sticas por cluster
âœ“ clusters_visualization.png   â†’ VisualizaciÃ³n 2D
âœ“ cluster_distribution.png     â†’ DistribuciÃ³n de libros
```

---

## Conceptos Clave para el Examen

### 1. **TF-IDF**

- Convierte texto â†’ nÃºmeros
- Da mÃ¡s importancia a palabras especÃ­ficas
- Reduce importancia de palabras comunes

### 2. **K-Means**

- Algoritmo de clustering
- Requiere definir K (nÃºmero de clusters)
- Minimiza distancia dentro de clusters

### 3. **AplicaciÃ³n**

- RecomendaciÃ³n de productos
- SegmentaciÃ³n de clientes
- OrganizaciÃ³n de documentos
- DetecciÃ³n de patrones

---

## Alternativa: DecisionTreeClassifier

El enunciado tambiÃ©n menciona clasificaciÃ³n de gÃ©nero. AquÃ­ la diferencia:

### Clustering (K-Means):

```python
# No necesitas etiquetas
kmeans = KMeans(n_clusters=6)
clusters = kmeans.fit_predict(tfidf_matrix)
```

### ClasificaciÃ³n (DecisionTree):

```python
# SÃ necesitas etiquetas (gÃ©neros conocidos)
X = tfidf_matrix
y = df['genres']  # Etiquetas conocidas

clf = DecisionTreeClassifier()
clf.fit(X, y)  # Entrenar
predicciones = clf.predict(X_nuevo)  # Predecir nuevos
```

**Para tu proyecto, K-Means es mÃ¡s apropiado** porque:

1. No tienes gÃ©neros limpios para todos los libros
2. Los gÃ©neros de Google Books son inconsistentes
3. El clustering descubrirÃ¡ grupos naturales

---

## Preguntas Frecuentes

**P: Â¿Por quÃ© 6 clusters?**
R: Es un balance. Puedes probar con 4-10 y elegir el mejor usando el "mÃ©todo del codo" (elbow method).

**P: Â¿QuÃ© pasa si un libro no tiene descripciÃ³n?**
R: Se elimina del anÃ¡lisis (no se puede clusterizar sin texto).

**P: Â¿Los clusters tienen nombres?**
R: No, K-Means solo da nÃºmeros (0, 1, 2...). TÃº les pones nombres analizando las palabras clave.

**P: Â¿Es mejor que clasificaciÃ³n?**
R: Depende. Clustering descubre patrones, clasificaciÃ³n necesita ejemplos previos.

---

## ðŸ“– Recursos Adicionales

- [DocumentaciÃ³n scikit-learn K-Means](https://scikit-learn.org/stable/modules/clustering.html#k-means)
- [TF-IDF explicado](https://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting)
- [VisualizaciÃ³n con PCA](https://scikit-learn.org/stable/modules/decomposition.html#pca)

---
